# Arguments that get passed to the constructor of your class instance
# as config={} keyword
environment:
    amount_of_gtps:         1
    amount_of_outputs:      2
    gtp_demand_size:        100
    process_time_at_GTP:    6       #time to process one product in sec
    speed_improvement:      0.00    #percent
    gtp_buffer_length:      7       #how much items fit in the buffer lane of the GTP workstation
    pipeline_length:        10
    in_que_observed:        5
    exception_occurence:    0.0     #how often an exception occurs
    termination_condition:  3       #1 = full demand satisfied, 2= queue demand satisfied, 3=terminate at max items processed
    max_items_processed:    150
    max_cycle_count:        400
    max_steps:              10000
    steps_by_heuristic:     20      #how many steps to start up by heuristic
    stochastic_demand:      True    #if set to false processing time of carriers is known upfront
    repurpose_goal:         False
    remove_cycles:          False
    warmstart_with_startstate: True    #True For DRL / heuristic, False when: testing different buffer size
    terminate_on_cycle:     False
    terminate_on_idle:      True
    alternative_terminate: False     #alternative termination conditions are:
                                        # (1). terminate when max steps reached,
                                        # (2). and terminate when max_cycle_count reached

    #for render
    render_width: 1280


    #carrier distribution
    percentage_small_carriers: 0.33
    percentage_medium_carriers: 0.34
    percentage_large_carriers: 0.33

    ###   observation: list including number of what to include
    # 1 - Full bottom conveyor: type for each location, binary
    # 2 - occupation of the output point (1/0)
    # 3 - Length of the queue, for each queue, normalized
    # 4 - observed demand, per queue, binary
    # 5 - Amount of items on conveyor, normalized
    # 6 - cycle factor (amount of cycles / max) normalized
    # 7 - usability of items on conveyor - factor (mean over amount of gtp)
    # 8 - remaining processing time of the queue (per queue), normalized
    # 9 - 2 -  queue can still take items, per queue (1/0)
    # 10 - 2 - queue is empty (< 2), per queue (1/0)
    # 11 - there is an item in the lead  for each type
    # 12 - 4 - in pipe number equal to 13
    # 13 - What is currently in pipe (of each type) normalized
    # 14 - Remaining processing  time of queue + workstation
    # 15 - Remaining waiting time per workstation
    # 16 - Remaining processing time of queue equal to 8
    # 17 - Remaining procesing time of items in pipeline
    # 18 - Number of items in pipeline, normalized

    #to try
    #observation_shape: [1,2,3,4,6,8,10,12]
    observation_shape: [4, 15, 16, 17, 18, 1, 2, 3, 6]
    #observation_shape: [2, 4, 9, 10, 12]
    #observation_shape: [1, 12 , 4, 9, 10]
    #observation_shape: [1, 3, 4, 9, 10, 12]
    #observation_shape: [2, 3, 4, 9, 10, 12]

    # Rewards
    # Reward fuction:
    # R = -(Delta Idle time * Idle time reward factor + delta cycle_count * cycle count reward factor)
    #where
    # Delta idle time = Sum(idle time per gtp)
    # Delta cycle count = 1 if cycle in step else 0
    # factors are:
    idle_time_reward_factor:            0
    cycle_count_reward_factor:          0
    output_priming_reward:              0.00
    delivery_reward:                    0

    #or for the reward shaping
    positive_reward_for_divert:          3
    wrong_sup_at_goal:                   0
    flooding_reward:                     0
    neg_reward_ia:                       0
    negative_reward_for_empty_queue:     0
    negative_reward_for_cycle:           3
    reward_terminate_idle:               6
    reward_towards_goal:                 0


main:
    # MODELS:
    # -----
    # For discrete action space environments:
    #     PPO2, DQN, ACER, A2C, ACKTR
    # For continuous action space environments:
    #     PPO2, A2C

    # POLICIES:
    # ---------
    #     Configurable:
    #         CustomCnnPolicy - CNN feature extraction
    #         CustomLSTMPolicy - LSTM cells followed by a multilayer perceptron
    #         CustomMlpPolicy - Multilayer perceptron
    #         CustomDQNPolicy - Multilayer perceptron specifically for DQN
    #     Defaults:
    #         CnnPolicy - CNN as described in 2014 Atari paper
    #         MlpPolicy - simple MLP with two hidden layers of size 64

    model:          PPO2
    policy:         MlpPolicy
    n_workers:      8           # Parallel environments
    n_steps:        15000000     # Steps to train
    save_every:     1000000      # Save a checkpoint of the model every n steps
    firsttrain:     2000000
    secondtrain:    2000000


    # Tensorboard logs for environment attributes e.g. self.steps
    logs:
        - steps
        - cycle_count
        - episode

models:
    PPO2:
        gamma:          0.99     # Discount factor for future rewards
        n_steps:        128         # Batch size (n_steps * n_workers)
        ent_coef:       0.01        # Entropy loss coefficient (higher values encourage more exploration)
        learning_rate:  0.00025     # LR
        vf_coef:        0.5         # The contribution of value function loss to the total loss of the network
        max_grad_norm:  0.3         # Max range of the gradient clipping
        lam:            0.95        # Generalized advantage estimation, for controlling variance/bias tradeoff
        nminibatches:   4           # Number of minibatches for SGD/Adam updates
        noptepochs:     4           # Number of iterations for SGD/Adam
        cliprange:      0.2         # Clip factor for PPO (the action probability distribution of the updated policy cannot differ from the old one by this fraction [measured by KL divergence])
        full_tensorboard_log: False
        verbose:        0

    DQN:
        gamma:          0.996
        learning_rate:  0.001
        buffer_size:    20000
        exploration_fraction: 0.1
        exploration_final_eps: 0.01
        train_freq:         1
        batch_size:         32
        learning_starts:        1000
        target_network_update_freq: 500
        prioritized_replay: false
        prioritized_replay_alpha: 0.2
        prioritized_replay_beta0: 0.4
        prioritized_replay_beta_iters: None
        prioritized_replay_eps: 0.000001
        param_noise: False
        verbose: 1
        full_tensorboard_log: False
        _init_setup_model: True
    A2C:
        gamma: 0.99
        learning_rate: 0.0007
        n_steps: 5
        vf_coef: 0.25
        ent_coef: 0.01
        max_grad_norm: 0.5
        alpha: 0.99
        epsilon: 0.0001
        lr_schedule: constant
        verbose: 0
        full_tensorboard_log: False
    ACER:
        gamma: 0.99
        n_steps: 20
        num_procs: 1
        q_coef: 0.5
        ent_coef: 0.01
        max_grad_norm: 10
        learning_rate: 0.0007
        lr_schedule: linear
        rprop_alpha: 0.99
        rprop_epsilon: 0.0001
        buffer_size: 5000
        replay_ratio: 4
        replay_start: 1000
        correction_term: 10.0
        trust_region: true
        alpha: 0.99
        delta: 1
        verbose: 0
    ACKTR:
        gamma: 0.99
        nprocs: 1
        n_steps: 20
        ent_coef: 0.01
        vf_coef: 0.25
        vf_fisher_coef: 1.0
        learning_rate: 0.25
        max_grad_norm: 0.5
        kfac_clip: 0.001
        lr_schedule: linear
        verbose: 0
        async_eigen_decomp: False
        full_tensorboard_log: False
policies:
    CustomMlpPolicy:
        shared:
            - 64
            - 64
        h_actor: [] # Policy head
            # - 16
        h_critic: [] # Value head
            # - 16

    CustomDQNPolicy:
        layers:
            - 64
            - 64

    CustomLSTMPolicy:
        n_lstm: 64
        shared:
            - 64
            - 64
            - lstm
        h_actor: []
            # - 16
        h_critic: []
            # - 16

    CustomCnnPolicy:
        filters:
        - 1
        # - 4
        # - 4
        kernel_size:
        - 3
        # - 3
        # - 3
        stride:
        - 1
        # - 1
        # - 1
        shared:
        - 128
        - 128
        h_actor: []
        h_critic: []
        activ: relu
        pd_init_scale: 0.05
        conv_init_scale: 1.4
        kernel_initializer: glorot_normal_initializer
        init_bias: .5


