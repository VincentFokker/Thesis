{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\_vinc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  if sys.path[0] == '':\n",
      "INFO:root:queues that are initialized: [[2, 3, 1, 2, 1, 1, 2, 3, 2, 1], [3, 1, 3, 2, 2, 1, 3, 3, 2, 1], [1, 2, 2, 2, 1, 2, 1, 3, 1, 3]]\n"
     ]
    }
   ],
   "source": [
    "from rl.environments.SimpleConveyor12 import SimpleConveyor12\n",
    "import yaml\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "config_path = 'rl/config/SimpleConveyor12.yml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.load(f)\n",
    "env = SimpleConveyor12(config)\n",
    "\n",
    "#modelpath = 'rl/trained_models/SimpleConveyor10/20201016_1100/0_WrapperCheck_1/model.zip'\n",
    "#model = PPO2.load(modelpath, env=DummyVecEnv([lambda: env]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the different parameters \n",
    "epsilon = 0.9\n",
    "total_episodes = 10000\n",
    "max_steps = 100\n",
    "alpha = 0.85\n",
    "gamma = 0.95\n",
    "  \n",
    "#Initializing the Q-matrix \n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "#Function to choose the next action \n",
    "def choose_action(state): \n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon: \n",
    "        action = env.action_space.sample() \n",
    "    else: \n",
    "        action = np.argmax(Q[state, :]) \n",
    "    return action \n",
    "  \n",
    "#Function to learn the Q-value \n",
    "def update(state, state2, reward, action, action2): \n",
    "    predict = Q[state, action] \n",
    "    target = reward + gamma * Q[state2, action2] \n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:    38, steps: 100, R: -1451.756\r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-388523cf044d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#Learning the Q-value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mstate1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-228a5cfe54b9>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(state, state2, reward, action, action2)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 18 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "#Initializing the reward \n",
    "reward=0\n",
    "  \n",
    "# Starting the SARSA learning \n",
    "for episode in range(total_episodes): \n",
    "    t = 0\n",
    "    state1 = env.reset() \n",
    "    action1 = choose_action(state1) \n",
    "  \n",
    "    while t < max_steps: \n",
    "        #Visualizing the training \n",
    "        #env.render() \n",
    "          \n",
    "        #Getting the next state \n",
    "        state2, reward, done, info = env.step(action1) \n",
    "  \n",
    "        #Choosing the next action \n",
    "        action2 = choose_action(state2) \n",
    "          \n",
    "        #Learning the Q-value \n",
    "        update(state1, state2, reward, action1, action2) \n",
    "  \n",
    "        state1 = state2 \n",
    "        action1 = action2 \n",
    "          \n",
    "        #Updating the respective vaLues \n",
    "        t += 1\n",
    "        reward += 1\n",
    "          \n",
    "        #If at the end of learning process \n",
    "        if done: \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word= ''\n",
    "for letter in env.make_observation():\n",
    "    word += str(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "562949953421312"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(word, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "th_new-venv",
   "language": "python",
   "name": "th_new-venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
